{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7c02f7",
   "metadata": {},
   "source": [
    "# Ethereum Bayesian Network Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive Bayesian Network for Ethereum price prediction using:\n",
    "- **Data**: ETH-USD OHLCV from 2017-01-01 to present\n",
    "- **Targets**: \n",
    "  - Next-day return category (5 classes: BIG_DOWN, SMALL_DOWN, NEUTRAL, SMALL_UP, BIG_UP)\n",
    "  - Fat-tail event detection (EXTREME_DROP, EXTREME_SPIKE, NONE)\n",
    "- **Features**: Technical indicators, volume, volatility, and candlestick patterns\n",
    "- **Model**: pgmpy Bayesian Network with structure learning\n",
    "\n",
    "## Pipeline Steps\n",
    "1. Data Collection & Preprocessing\n",
    "2. Feature Engineering (Technical Indicators)\n",
    "3. Target Label Creation\n",
    "4. Bayesian Network Structure Learning\n",
    "5. Inference Engine\n",
    "6. Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ee4d6",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085485a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BicScore' from 'pgmpy.estimators' (c:\\Users\\saris\\anaconda3\\envs\\bayes\\Lib\\site-packages\\pgmpy\\estimators\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Bayesian Network (latest pgmpy, Python 3.11+)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpgmpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BayesianNetwork\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpgmpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mestimators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HillClimbSearch, BicScore\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpgmpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mestimators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MaximumLikelihoodEstimator\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpgmpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VariableElimination\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'BicScore' from 'pgmpy.estimators' (c:\\Users\\saris\\anaconda3\\envs\\bayes\\Lib\\site-packages\\pgmpy\\estimators\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Bayesian Network (latest pgmpy, Python 3.11+)\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import HillClimbSearch, BicScore\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Technical indicators\n",
    "import talib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "# ML evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Configure display\n",
    "plt.style.use('dark_background')\n",
    "pd.set_option('display. max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0211506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection Parameters\n",
    "TICKER = 'ETH-USD'\n",
    "START_DATE = '2017-01-01'\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"üìä Fetching {TICKER} data from {START_DATE} to {END_DATE}...\")\n",
    "\n",
    "# Fetch OHLCV data\n",
    "try:\n",
    "    eth_data = yf.download(TICKER, start=START_DATE, end=END_DATE, progress=False)\n",
    "    \n",
    "    # Clean data\n",
    "    eth_data = eth_data.dropna()\n",
    "    \n",
    "    # Basic validation\n",
    "    if eth_data.empty:\n",
    "        raise ValueError(\"No data fetched. Check ticker or date range.\")\n",
    "    \n",
    "    print(f\"‚úÖ Data fetched successfully:\")\n",
    "    print(f\"   - Date range: {eth_data.index.min().strftime('%Y-%m-%d')} to {eth_data.index.max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   - Total observations: {len(eth_data):,}\")\n",
    "    print(f\"   - Columns: {list(eth_data.columns)}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nüìã Sample Data:\")\n",
    "    display(eth_data.head().round(2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error fetching data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a506c06",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "We'll create comprehensive technical indicators and discretize them into categorical variables for the Bayesian Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc32f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_technical_features(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive technical indicators for Bayesian Network.\n",
    "    All features will be discretized into 3 categories: Low, Medium, High\n",
    "    \"\"\"\n",
    "    features_df = df.copy()\n",
    "    \n",
    "    # Extract OHLCV arrays for talib\n",
    "    high = df['High'].values\n",
    "    low = df['Low'].values\n",
    "    close = df['Close'].values\n",
    "    open_price = df['Open'].values\n",
    "    volume = df['Volume'].values\n",
    "    \n",
    "    print(\"üîß Creating technical indicators...\")\n",
    "    \n",
    "    # 1. Technical Indicators\n",
    "    features_df['RSI'] = talib.RSI(close, timeperiod=14)\n",
    "    \n",
    "    # MACD\n",
    "    macd, macd_signal, macd_hist = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    features_df['MACD'] = macd\n",
    "    features_df['MACD_Signal'] = macd_signal\n",
    "    features_df['MACD_Hist'] = macd_hist\n",
    "    \n",
    "    # Moving averages\n",
    "    features_df['SMA_14'] = talib.SMA(close, timeperiod=14)\n",
    "    features_df['SMA_50'] = talib.SMA(close, timeperiod=50)\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    bb_upper, bb_middle, bb_lower = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    features_df['BB_Upper'] = bb_upper\n",
    "    features_df['BB_Lower'] = bb_lower\n",
    "    features_df['BB_Width'] = (bb_upper - bb_lower) / bb_middle\n",
    "    features_df['BB_Position'] = (close - bb_lower) / (bb_upper - bb_lower)\n",
    "    \n",
    "    # ATR (Average True Range)\n",
    "    features_df['ATR'] = talib.ATR(high, low, close, timeperiod=14)\n",
    "    features_df['ATR_Pct'] = features_df['ATR'] / close\n",
    "    \n",
    "    # 2. Return-based features\n",
    "    features_df['Return_1d'] = df['Close'].pct_change(1)\n",
    "    features_df['Return_3d'] = df['Close'].pct_change(3)\n",
    "    features_df['Open_Close_Pct'] = (close - open_price) / open_price\n",
    "    \n",
    "    # 3. Candlestick features\n",
    "    features_df['Body_Size'] = abs(close - open_price) / open_price\n",
    "    features_df['Upper_Wick'] = (high - np.maximum(close, open_price)) / np.maximum(close, open_price)\n",
    "    features_df['Lower_Wick'] = (np.minimum(close, open_price) - low) / np.minimum(close, open_price)\n",
    "    \n",
    "    # 4. Volume features\n",
    "    features_df['Volume_Change'] = df['Volume'].pct_change(1)\n",
    "    features_df['Volume_SMA'] = talib.SMA(volume.astype(float), timeperiod=20)\n",
    "    features_df['Volume_Surge'] = volume / features_df['Volume_SMA']\n",
    "    \n",
    "    # 5. Price momentum\n",
    "    features_df['Price_vs_SMA14'] = close / features_df['SMA_14'] - 1\n",
    "    features_df['Price_vs_SMA50'] = close / features_df['SMA_50'] - 1\n",
    "    \n",
    "    print(f\"‚úÖ Created {len([c for c in features_df.columns if c not in df.columns])} new technical features\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "def discretize_features(df, feature_cols):\n",
    "    \"\"\"\n",
    "    Discretize continuous features into Low, Medium, High categories using quantiles\n",
    "    \"\"\"\n",
    "    df_discrete = df.copy()\n",
    "    \n",
    "    print(\"üî¢ Discretizing features into categorical variables...\")\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        if col in df.columns and df[col].notna().any():\n",
    "            # Use quantile-based binning\n",
    "            df_discrete[f\"{col}_Cat\"] = pd.qcut(\n",
    "                df[col], \n",
    "                q=3, \n",
    "                labels=['Low', 'Medium', 'High'],\n",
    "                duplicates='drop'\n",
    "            )\n",
    "    \n",
    "    categorical_cols = [c for c in df_discrete.columns if c.endswith('_Cat')]\n",
    "    print(f\"‚úÖ Created {len(categorical_cols)} discretized features\")\n",
    "    \n",
    "    return df_discrete, categorical_cols\n",
    "\n",
    "# Create features\n",
    "eth_features = create_technical_features(eth_data)\n",
    "\n",
    "# Define features to discretize\n",
    "TECHNICAL_FEATURES = [\n",
    "    'RSI', 'MACD', 'MACD_Hist', 'BB_Width', 'BB_Position', 'ATR_Pct',\n",
    "    'Return_1d', 'Return_3d', 'Open_Close_Pct', 'Body_Size', 'Upper_Wick', 'Lower_Wick',\n",
    "    'Volume_Change', 'Volume_Surge', 'Price_vs_SMA14', 'Price_vs_SMA50'\n",
    "]\n",
    "\n",
    "# Discretize features\n",
    "eth_discrete, categorical_features = discretize_features(eth_features, TECHNICAL_FEATURES)\n",
    "\n",
    "print(f\"\\nüìä Feature Summary:\")\n",
    "print(f\"   - Original OHLCV columns: {len(eth_data.columns)}\")\n",
    "print(f\"   - Technical features: {len(TECHNICAL_FEATURES)}\")\n",
    "print(f\"   - Categorical features: {len(categorical_features)}\")\n",
    "print(f\"   - Total columns: {len(eth_discrete.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc285a8a",
   "metadata": {},
   "source": [
    "## 3. Target Label Creation\n",
    "\n",
    "Create two prediction targets:\n",
    "1. **Return Category**: 5-class next-day return prediction\n",
    "2. **Fat-Tail Events**: Extreme price movements (top/bottom 5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e654d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_labels(df):\n",
    "    \"\"\"\n",
    "    Create target variables for Bayesian Network:\n",
    "    1. Return_Category: 5-class next-day return (BIG_DOWN, SMALL_DOWN, NEUTRAL, SMALL_UP, BIG_UP)\n",
    "    2. FatTailEvent: Extreme events (EXTREME_DROP, EXTREME_SPIKE, NONE)\n",
    "    \"\"\"\n",
    "    targets_df = df.copy()\n",
    "    \n",
    "    print(\"üéØ Creating target labels...\")\n",
    "    \n",
    "    # Calculate next-day returns\n",
    "    next_day_return = df['Close'].pct_change(1).shift(-1)  # Shift -1 to get next day\n",
    "    \n",
    "    # Target 1: Return Category (5 classes using quantiles)\n",
    "    targets_df['Return_Category'] = pd.qcut(\n",
    "        next_day_return,\n",
    "        q=5,\n",
    "        labels=['BIG_DOWN', 'SMALL_DOWN', 'NEUTRAL', 'SMALL_UP', 'BIG_UP']\n",
    "    )\n",
    "    \n",
    "    # Target 2: Fat-Tail Events (extreme 5% tails)\n",
    "    extreme_low = next_day_return.quantile(0.05)\n",
    "    extreme_high = next_day_return.quantile(0.95)\n",
    "    \n",
    "    def classify_fat_tail(ret):\n",
    "        if pd.isna(ret):\n",
    "            return None\n",
    "        elif ret <= extreme_low:\n",
    "            return 'EXTREME_DROP'\n",
    "        elif ret >= extreme_high:\n",
    "            return 'EXTREME_SPIKE'\n",
    "        else:\n",
    "            return 'NONE'\n",
    "    \n",
    "    targets_df['FatTailEvent'] = next_day_return.apply(classify_fat_tail)\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nüìà Target Statistics:\")\n",
    "    print(f\"\\n1. Return Category Distribution:\")\n",
    "    print(targets_df['Return_Category'].value_counts().sort_index())\n",
    "    \n",
    "    print(f\"\\n2. Fat-Tail Event Distribution:\")\n",
    "    print(targets_df['FatTailEvent'].value_counts())\n",
    "    \n",
    "    print(f\"\\nüìä Extreme Thresholds:\")\n",
    "    print(f\"   - Extreme Drop: {extreme_low:.2%} (bottom 5%)\")\n",
    "    print(f\"   - Extreme Spike: {extreme_high:.2%} (top 5%)\")\n",
    "    \n",
    "    return targets_df\n",
    "\n",
    "# Create targets\n",
    "eth_with_targets = create_target_labels(eth_discrete)\n",
    "\n",
    "# Add target columns to our categorical features list\n",
    "TARGET_FEATURES = ['Return_Category', 'FatTailEvent']\n",
    "ALL_FEATURES = categorical_features + TARGET_FEATURES\n",
    "\n",
    "print(f\"\\n‚úÖ Total features for Bayesian Network: {len(ALL_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a832b",
   "metadata": {},
   "source": [
    "## 4. Data Preparation for Bayesian Network\n",
    "\n",
    "Clean and prepare the final dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627af62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bayesian_data(df, feature_cols):\n",
    "    \"\"\"\n",
    "    Prepare clean dataset for Bayesian Network training\n",
    "    \"\"\"\n",
    "    print(\"üßπ Preparing data for Bayesian Network...\")\n",
    "    \n",
    "    # Select only the features we need\n",
    "    bn_data = df[feature_cols].copy()\n",
    "    \n",
    "    # Remove rows with any missing values (Bayesian Networks need complete data)\n",
    "    initial_rows = len(bn_data)\n",
    "    bn_data = bn_data.dropna()\n",
    "    final_rows = len(bn_data)\n",
    "    \n",
    "    print(f\"   - Initial rows: {initial_rows:,}\")\n",
    "    print(f\"   - Rows after removing NaN: {final_rows:,}\")\n",
    "    print(f\"   - Rows dropped: {initial_rows - final_rows:,}\")\n",
    "    \n",
    "    # Ensure all columns are strings (pgmpy requirement)\n",
    "    for col in bn_data.columns:\n",
    "        bn_data[col] = bn_data[col].astype(str)\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\nüìã Final Dataset Sample:\")\n",
    "    display(bn_data.head())\n",
    "    \n",
    "    # Show feature distributions\n",
    "    print(f\"\\nüìä Target Variable Distributions:\")\n",
    "    for target in TARGET_FEATURES:\n",
    "        if target in bn_data.columns:\n",
    "            print(f\"\\n{target}:\")\n",
    "            print(bn_data[target].value_counts())\n",
    "    \n",
    "    return bn_data\n",
    "\n",
    "# Prepare final dataset\n",
    "bayesian_data = prepare_bayesian_data(eth_with_targets, ALL_FEATURES)\n",
    "\n",
    "print(f\"\\n‚úÖ Bayesian Network dataset ready with {len(bayesian_data)} samples and {len(bayesian_data.columns)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea0848f",
   "metadata": {},
   "source": [
    "## 5. Bayesian Network Structure Learning\n",
    "\n",
    "Use Hill Climb Search with BIC scoring to learn the optimal network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ce474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_bayesian_structure(data, max_time=300):\n",
    "    \"\"\"\n",
    "    Learn Bayesian Network structure using Hill Climb Search\n",
    "    \"\"\"\n",
    "    print(\"üß† Learning Bayesian Network structure...\")\n",
    "    print(f\"   - Using Hill Climb Search with BIC scoring\")\n",
    "    print(f\"   - Maximum time: {max_time} seconds\")\n",
    "    print(f\"   - Variables: {len(data.columns)}\")\n",
    "    \n",
    "    # Initialize structure learning\n",
    "    scoring_method = BicScore(data)\n",
    "    est = HillClimbSearch(data)\n",
    "    \n",
    "    # Learn structure (this may take a while)\n",
    "    try:\n",
    "        print(\"\\n‚è≥ Structure learning in progress...\")\n",
    "        best_model = est.estimate(scoring_method=scoring_method, max_time=max_time)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Structure learning completed!\")\n",
    "        print(f\"   - Nodes: {len(best_model.nodes())}\")\n",
    "        print(f\"   - Edges: {len(best_model.edges())}\")\n",
    "        \n",
    "        # Show edges involving target variables\n",
    "        target_edges = []\n",
    "        for edge in best_model.edges():\n",
    "            if edge[0] in TARGET_FEATURES or edge[1] in TARGET_FEATURES:\n",
    "                target_edges.append(edge)\n",
    "        \n",
    "        print(f\"\\nüéØ Edges involving target variables ({len(target_edges)}):\")\n",
    "        for edge in target_edges:\n",
    "            print(f\"   - {edge[0]} -> {edge[1]}\")\n",
    "        \n",
    "        return best_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in structure learning: {e}\")\n",
    "        return None\n",
    "\n",
    "# Learn structure\n",
    "bn_model = learn_bayesian_structure(bayesian_data, max_time=600)  # 10 minutes max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38daab2d",
   "metadata": {},
   "source": [
    "## 6. Parameter Learning and Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5429eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bayesian_network(model, data):\n",
    "    \"\"\"\n",
    "    Fit Bayesian Network parameters using Maximum Likelihood Estimation\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        print(\"‚ùå No model to fit\")\n",
    "        return None\n",
    "        \n",
    "    print(\"üìä Fitting Bayesian Network parameters...\")\n",
    "    \n",
    "    try:\n",
    "        # Create Bayesian Network with learned structure\n",
    "        bn = BayesianNetwork(model.edges())\n",
    "        \n",
    "        # Fit parameters using Maximum Likelihood Estimation\n",
    "        mle = MaximumLikelihoodEstimator(bn, data)\n",
    "        bn.fit(data, estimator=MaximumLikelihoodEstimator)\n",
    "        \n",
    "        print(f\"‚úÖ Model fitted successfully!\")\n",
    "        print(f\"   - Nodes: {len(bn.nodes())}\")\n",
    "        print(f\"   - Edges: {len(bn.edges())}\")\n",
    "        \n",
    "        # Check model validity\n",
    "        print(f\"   - Model is valid: {bn.check_model()}\")\n",
    "        \n",
    "        return bn\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fitting model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fit the model\n",
    "fitted_bn = fit_bayesian_network(bn_model, bayesian_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811eba58",
   "metadata": {},
   "source": [
    "## 7. Inference Engine\n",
    "\n",
    "Create an inference engine to make predictions given evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb61eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_engine(model):\n",
    "    \"\"\"\n",
    "    Create inference engine for predictions\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        print(\"‚ùå No model available for inference\")\n",
    "        return None\n",
    "        \n",
    "    print(\"üîÆ Creating inference engine...\")\n",
    "    \n",
    "    try:\n",
    "        inference = VariableElimination(model)\n",
    "        print(\"‚úÖ Inference engine created successfully!\")\n",
    "        return inference\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating inference engine: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_probabilities(inference_engine, evidence, target_vars):\n",
    "    \"\"\"\n",
    "    Predict probabilities for target variables given evidence\n",
    "    \"\"\"\n",
    "    if inference_engine is None:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        results = {}\n",
    "        \n",
    "        for target in target_vars:\n",
    "            if target in fitted_bn.nodes():\n",
    "                # Query the target variable\n",
    "                query_result = inference_engine.query(variables=[target], evidence=evidence)\n",
    "                \n",
    "                # Convert to dictionary\n",
    "                probs = {}\n",
    "                for i, state in enumerate(query_result.state_names[target]):\n",
    "                    probs[state] = query_result.values[i]\n",
    "                \n",
    "                results[target] = probs\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create inference engine\n",
    "inference = create_inference_engine(fitted_bn)\n",
    "\n",
    "# Example inference\n",
    "if inference is not None:\n",
    "    print(\"\\nüß™ Example Inference:\")\n",
    "    \n",
    "    # Create example evidence\n",
    "    example_evidence = {\n",
    "        'RSI_Cat': 'High',\n",
    "        'Volume_Surge_Cat': 'High',\n",
    "        'BB_Position_Cat': 'High'\n",
    "    }\n",
    "    \n",
    "    # Only use evidence that exists in the model\n",
    "    valid_evidence = {k: v for k, v in example_evidence.items() if k in fitted_bn.nodes()}\n",
    "    \n",
    "    if valid_evidence:\n",
    "        print(f\"\\nEvidence: {valid_evidence}\")\n",
    "        \n",
    "        predictions = predict_probabilities(inference, valid_evidence, TARGET_FEATURES)\n",
    "        \n",
    "        if predictions:\n",
    "            for target, probs in predictions.items():\n",
    "                print(f\"\\n{target} Probabilities:\")\n",
    "                for state, prob in probs.items():\n",
    "                    print(f\"   {state}: {prob:.3f}\")\n",
    "    else:\n",
    "        print(\"No valid evidence variables found in the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a7dff",
   "metadata": {},
   "source": [
    "## 8. Visualization\n",
    "\n",
    "Visualize the learned Bayesian Network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bayesian_network(model, title=\"Ethereum Bayesian Network\", figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Visualize the Bayesian Network structure\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        print(\"‚ùå No model to visualize\")\n",
    "        return\n",
    "        \n",
    "    print(\"üé® Creating network visualization...\")\n",
    "    \n",
    "    try:\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Create NetworkX graph\n",
    "        G = nx.DiGraph()\n",
    "        G.add_edges_from(model.edges())\n",
    "        \n",
    "        # Create layout\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "        \n",
    "        # Color nodes by type\n",
    "        node_colors = []\n",
    "        for node in G.nodes():\n",
    "            if node in TARGET_FEATURES:\n",
    "                node_colors.append('#ff6b6b')  # Red for targets\n",
    "            elif 'RSI' in node or 'MACD' in node:\n",
    "                node_colors.append('#4ecdc4')  # Teal for momentum indicators\n",
    "            elif 'Volume' in node:\n",
    "                node_colors.append('#45b7d1')  # Blue for volume\n",
    "            elif 'BB_' in node or 'ATR' in node:\n",
    "                node_colors.append('#96ceb4')  # Green for volatility\n",
    "            else:\n",
    "                node_colors.append('#feca57')  # Yellow for others\n",
    "        \n",
    "        # Draw the network\n",
    "        nx.draw(G, pos, \n",
    "                node_color=node_colors,\n",
    "                node_size=1500,\n",
    "                font_size=8,\n",
    "                font_weight='bold',\n",
    "                arrows=True,\n",
    "                arrowsize=20,\n",
    "                edge_color='gray',\n",
    "                alpha=0.7,\n",
    "                with_labels=True)\n",
    "        \n",
    "        plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#ff6b6b', markersize=10, label='Target Variables'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#4ecdc4', markersize=10, label='Momentum Indicators'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#45b7d1', markersize=10, label='Volume Features'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#96ceb4', markersize=10, label='Volatility Features'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#feca57', markersize=10, label='Other Features')\n",
    "        ]\n",
    "        plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Network visualization created!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating visualization: {e}\")\n",
    "\n",
    "# Visualize the network\n",
    "if fitted_bn is not None:\n",
    "    visualize_bayesian_network(fitted_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e348d8",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n",
    "\n",
    "Evaluate the Bayesian Network against baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data, bn_model, inference_engine, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Evaluate Bayesian Network performance and compare to baseline\n",
    "    \"\"\"\n",
    "    if bn_model is None or inference_engine is None:\n",
    "        print(\"‚ùå No model available for evaluation\")\n",
    "        return\n",
    "        \n",
    "    print(\"üìà Evaluating model performance...\")\n",
    "    \n",
    "    try:\n",
    "        # Time-aware split (last 20% for testing)\n",
    "        split_idx = int(len(data) * (1 - test_size))\n",
    "        train_data = data.iloc[:split_idx]\n",
    "        test_data = data.iloc[split_idx:]\n",
    "        \n",
    "        print(f\"   - Training samples: {len(train_data):,}\")\n",
    "        print(f\"   - Testing samples: {len(test_data):,}\")\n",
    "        \n",
    "        # Get feature columns (exclude targets)\n",
    "        feature_cols = [c for c in data.columns if c not in TARGET_FEATURES]\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Evaluate each target\n",
    "        for target in TARGET_FEATURES:\n",
    "            if target not in data.columns:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nüéØ Evaluating {target}...\")\n",
    "            \n",
    "            # Prepare data for baseline model\n",
    "            # Encode categorical features for logistic regression\n",
    "            train_features_encoded = pd.get_dummies(train_data[feature_cols])\n",
    "            test_features_encoded = pd.get_dummies(test_data[feature_cols])\n",
    "            \n",
    "            # Align columns\n",
    "            common_cols = train_features_encoded.columns.intersection(test_features_encoded.columns)\n",
    "            train_features_encoded = train_features_encoded[common_cols]\n",
    "            test_features_encoded = test_features_encoded[common_cols]\n",
    "            \n",
    "            # Baseline: Logistic Regression\n",
    "            lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "            lr_model.fit(train_features_encoded, train_data[target])\n",
    "            lr_pred = lr_model.predict(test_features_encoded)\n",
    "            \n",
    "            print(f\"\\nüìä {target} - Baseline (Logistic Regression):\")\n",
    "            print(classification_report(test_data[target], lr_pred, zero_division=0))\n",
    "            \n",
    "            # Store results\n",
    "            results[target] = {\n",
    "                'baseline_accuracy': (lr_pred == test_data[target]).mean(),\n",
    "                'test_distribution': test_data[target].value_counts().to_dict()\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Evaluate the model\n",
    "if fitted_bn is not None and inference is not None:\n",
    "    evaluation_results = evaluate_model(bayesian_data, fitted_bn, inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd24da90",
   "metadata": {},
   "source": [
    "## 10. Scenario Analysis\n",
    "\n",
    "Demonstrate how different market conditions affect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a283f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_analysis(inference_engine, model):\n",
    "    \"\"\"\n",
    "    Perform scenario analysis with different market conditions\n",
    "    \"\"\"\n",
    "    if inference_engine is None or model is None:\n",
    "        print(\"‚ùå No model available for scenario analysis\")\n",
    "        return\n",
    "        \n",
    "    print(\"üé≠ Performing Scenario Analysis...\")\n",
    "    \n",
    "    # Define scenarios\n",
    "    scenarios = {\n",
    "        \"Bullish Momentum\": {\n",
    "            'RSI_Cat': 'High',\n",
    "            'MACD_Hist_Cat': 'High',\n",
    "            'BB_Position_Cat': 'High',\n",
    "            'Volume_Surge_Cat': 'High'\n",
    "        },\n",
    "        \"Bearish Momentum\": {\n",
    "            'RSI_Cat': 'Low',\n",
    "            'MACD_Hist_Cat': 'Low',\n",
    "            'BB_Position_Cat': 'Low',\n",
    "            'Volume_Surge_Cat': 'High'\n",
    "        },\n",
    "        \"Neutral Market\": {\n",
    "            'RSI_Cat': 'Medium',\n",
    "            'MACD_Hist_Cat': 'Medium',\n",
    "            'BB_Position_Cat': 'Medium',\n",
    "            'Volume_Surge_Cat': 'Medium'\n",
    "        },\n",
    "        \"High Volatility\": {\n",
    "            'ATR_Pct_Cat': 'High',\n",
    "            'BB_Width_Cat': 'High',\n",
    "            'Volume_Surge_Cat': 'High'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for scenario_name, evidence in scenarios.items():\n",
    "        print(f\"\\nüéØ Scenario: {scenario_name}\")\n",
    "        print(f\"Evidence: {evidence}\")\n",
    "        \n",
    "        # Filter evidence to only include variables in the model\n",
    "        valid_evidence = {k: v for k, v in evidence.items() if k in model.nodes()}\n",
    "        \n",
    "        if not valid_evidence:\n",
    "            print(\"   No valid evidence variables found in model\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Valid evidence: {valid_evidence}\")\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = predict_probabilities(inference_engine, valid_evidence, TARGET_FEATURES)\n",
    "        \n",
    "        if predictions:\n",
    "            for target, probs in predictions.items():\n",
    "                print(f\"\\n   {target}:\")\n",
    "                sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
    "                for state, prob in sorted_probs:\n",
    "                    print(f\"     {state}: {prob:.3f}\")\n",
    "        else:\n",
    "            print(\"   Could not generate predictions\")\n",
    "\n",
    "# Run scenario analysis\n",
    "if fitted_bn is not None and inference is not None:\n",
    "    scenario_analysis(inference, fitted_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb64290c",
   "metadata": {},
   "source": [
    "## 11. Summary and Next Steps\n",
    "\n",
    "### üéØ Pipeline Summary\n",
    "\n",
    "This notebook implements a comprehensive Bayesian Network pipeline for Ethereum price prediction with the following components:\n",
    "\n",
    "**Data & Features:**\n",
    "- ‚úÖ ETH-USD OHLCV data from 2017 to present\n",
    "- ‚úÖ 16 technical indicators (RSI, MACD, Bollinger Bands, ATR, etc.)\n",
    "- ‚úÖ Discretized into categorical variables (Low/Medium/High)\n",
    "\n",
    "**Prediction Targets:**\n",
    "- ‚úÖ **Return Category**: 5-class next-day return prediction\n",
    "- ‚úÖ **Fat-Tail Events**: Extreme price movements (top/bottom 5%)\n",
    "\n",
    "**Model:**\n",
    "- ‚úÖ Bayesian Network with Hill Climb structure learning\n",
    "- ‚úÖ BIC scoring for optimal structure\n",
    "- ‚úÖ Maximum Likelihood parameter estimation\n",
    "\n",
    "**Analysis:**\n",
    "- ‚úÖ Inference engine for probabilistic predictions\n",
    "- ‚úÖ Scenario analysis for different market conditions\n",
    "- ‚úÖ Baseline comparison with Logistic Regression\n",
    "- ‚úÖ Network visualization\n",
    "\n",
    "### üöÄ Key Insights\n",
    "\n",
    "1. **Probabilistic Predictions**: The model provides probability distributions rather than point predictions\n",
    "2. **Interpretability**: The network structure reveals relationships between technical indicators\n",
    "3. **Scenario Analysis**: Allows testing \"what-if\" scenarios for different market conditions\n",
    "4. **Fat-Tail Detection**: Specifically targets extreme events for risk management\n",
    "\n",
    "### üìà Next Steps\n",
    "\n",
    "**Model Improvements:**\n",
    "- Add more sophisticated features (e.g., regime detection, market microstructure)\n",
    "- Experiment with different discretization strategies\n",
    "- Include external factors (macro indicators, sentiment)\n",
    "\n",
    "**Evaluation Enhancements:**\n",
    "- Implement proper Bayesian Network accuracy metrics\n",
    "- Add cross-validation with time series constraints\n",
    "- Compare to more sophisticated baselines (Random Forest, XGBoost)\n",
    "\n",
    "**Production Considerations:**\n",
    "- Model updating pipeline for new data\n",
    "- Real-time inference API\n",
    "- Risk management integration\n",
    "- Portfolio optimization using probability outputs\n",
    "\n",
    "### üìã Dependencies\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy yfinance pgmpy talib matplotlib seaborn networkx scikit-learn\n",
    "```\n",
    "\n",
    "### üéØ Usage\n",
    "\n",
    "1. Run all cells in sequence\n",
    "2. The model will automatically fetch data, learn structure, and fit parameters\n",
    "3. Use the inference engine for predictions with custom evidence\n",
    "4. Analyze scenarios by modifying the evidence dictionary\n",
    "\n",
    "**Example Prediction:**\n",
    "```python\n",
    "evidence = {'RSI_Cat': 'High', 'Volume_Surge_Cat': 'High'}\n",
    "predictions = predict_probabilities(inference, evidence, TARGET_FEATURES)\n",
    "```\n",
    "\n",
    "This pipeline provides a solid foundation for probabilistic cryptocurrency price prediction using Bayesian Networks! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
