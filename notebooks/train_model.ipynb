{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f6b3f208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "Cuda version: 11.8\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and set up config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import random\n",
    "# Data pipeline: Load, join, align, feature engineering, targets, normalization, and sequence prep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from model_def import DeltaSenseTransformer\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch version:', torch.__version__)\n",
    "print('Cuda version:', torch.version.cuda)\n",
    "print('Using device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42332b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0628a02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping all-nan columns: (16493, 1742)\n",
      "after dropping all-nan columns: (16493, 1680)\n",
      "after dropping all-nan rows: (12509, 1680)\n",
      "start date: 2024-01-27 13:00:00+00:00 end date: 2025-07-01 17:00:00+00:00\n",
      "after dropping all-nan rows: (12509, 1680)\n",
      "start date: 2024-01-27 13:00:00+00:00 end date: 2025-07-01 17:00:00+00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1h_BTC-USD_Close</th>\n",
       "      <th>1h_BTC-USD_High</th>\n",
       "      <th>1h_BTC-USD_Low</th>\n",
       "      <th>1h_BTC-USD_Open</th>\n",
       "      <th>1h_BTC-USD_Volume</th>\n",
       "      <th>1h_BTC-USD_Pct_Change_1</th>\n",
       "      <th>1h_BTC-USD_Pct_Change_5</th>\n",
       "      <th>1h_BTC-USD_Pct_Change_10</th>\n",
       "      <th>1h_BTC-USD_SMA_10</th>\n",
       "      <th>1h_BTC-USD_SMA_20</th>\n",
       "      <th>...</th>\n",
       "      <th>1h_VXZ_BB_upper</th>\n",
       "      <th>1h_VXZ_BB_lower</th>\n",
       "      <th>1h_VXZ_RSI</th>\n",
       "      <th>1h_VXZ_OBV</th>\n",
       "      <th>1h_VXZ_ATR</th>\n",
       "      <th>1h_VXZ_MFI</th>\n",
       "      <th>1h_VXZ_Hist_Volatility</th>\n",
       "      <th>1h_VXZ_Donchian_Upper</th>\n",
       "      <th>1h_VXZ_Donchian_Lower</th>\n",
       "      <th>1h_VXZ_Z_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-07-01 16:00:00+00:00</th>\n",
       "      <td>106075.531250</td>\n",
       "      <td>106282.726562</td>\n",
       "      <td>105926.562500</td>\n",
       "      <td>106027.320312</td>\n",
       "      <td>1.142259e+10</td>\n",
       "      <td>0.029940</td>\n",
       "      <td>-0.502636</td>\n",
       "      <td>-0.787414</td>\n",
       "      <td>106495.921875</td>\n",
       "      <td>106812.092969</td>\n",
       "      <td>...</td>\n",
       "      <td>58.537098</td>\n",
       "      <td>57.645352</td>\n",
       "      <td>67.354319</td>\n",
       "      <td>-1413108.0</td>\n",
       "      <td>0.152536</td>\n",
       "      <td>71.730843</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>58.43</td>\n",
       "      <td>57.75</td>\n",
       "      <td>0.824345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-01 17:00:00+00:00</th>\n",
       "      <td>106219.570312</td>\n",
       "      <td>106315.867188</td>\n",
       "      <td>105958.820312</td>\n",
       "      <td>106114.359375</td>\n",
       "      <td>6.050824e+09</td>\n",
       "      <td>0.135789</td>\n",
       "      <td>-0.228747</td>\n",
       "      <td>-0.895195</td>\n",
       "      <td>106399.975781</td>\n",
       "      <td>106764.118750</td>\n",
       "      <td>...</td>\n",
       "      <td>58.537098</td>\n",
       "      <td>57.645352</td>\n",
       "      <td>67.354319</td>\n",
       "      <td>-1413108.0</td>\n",
       "      <td>0.152536</td>\n",
       "      <td>71.730843</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>58.43</td>\n",
       "      <td>57.75</td>\n",
       "      <td>0.824345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 1680 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           1h_BTC-USD_Close  1h_BTC-USD_High  1h_BTC-USD_Low  \\\n",
       "2025-07-01 16:00:00+00:00     106075.531250    106282.726562   105926.562500   \n",
       "2025-07-01 17:00:00+00:00     106219.570312    106315.867188   105958.820312   \n",
       "\n",
       "                           1h_BTC-USD_Open  1h_BTC-USD_Volume  \\\n",
       "2025-07-01 16:00:00+00:00    106027.320312       1.142259e+10   \n",
       "2025-07-01 17:00:00+00:00    106114.359375       6.050824e+09   \n",
       "\n",
       "                           1h_BTC-USD_Pct_Change_1  1h_BTC-USD_Pct_Change_5  \\\n",
       "2025-07-01 16:00:00+00:00                 0.029940                -0.502636   \n",
       "2025-07-01 17:00:00+00:00                 0.135789                -0.228747   \n",
       "\n",
       "                           1h_BTC-USD_Pct_Change_10  1h_BTC-USD_SMA_10  \\\n",
       "2025-07-01 16:00:00+00:00                 -0.787414      106495.921875   \n",
       "2025-07-01 17:00:00+00:00                 -0.895195      106399.975781   \n",
       "\n",
       "                           1h_BTC-USD_SMA_20  ...  1h_VXZ_BB_upper  \\\n",
       "2025-07-01 16:00:00+00:00      106812.092969  ...        58.537098   \n",
       "2025-07-01 17:00:00+00:00      106764.118750  ...        58.537098   \n",
       "\n",
       "                           1h_VXZ_BB_lower  1h_VXZ_RSI  1h_VXZ_OBV  \\\n",
       "2025-07-01 16:00:00+00:00        57.645352   67.354319  -1413108.0   \n",
       "2025-07-01 17:00:00+00:00        57.645352   67.354319  -1413108.0   \n",
       "\n",
       "                           1h_VXZ_ATR  1h_VXZ_MFI  1h_VXZ_Hist_Volatility  \\\n",
       "2025-07-01 16:00:00+00:00    0.152536   71.730843                0.007874   \n",
       "2025-07-01 17:00:00+00:00    0.152536   71.730843                0.007874   \n",
       "\n",
       "                           1h_VXZ_Donchian_Upper  1h_VXZ_Donchian_Lower  \\\n",
       "2025-07-01 16:00:00+00:00                  58.43                  57.75   \n",
       "2025-07-01 17:00:00+00:00                  58.43                  57.75   \n",
       "\n",
       "                           1h_VXZ_Z_Score  \n",
       "2025-07-01 16:00:00+00:00        0.824345  \n",
       "2025-07-01 17:00:00+00:00        0.824345  \n",
       "\n",
       "[2 rows x 1680 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TARGET_COL = '1h_ETH-USD_Close'\n",
    "\n",
    "crypto = pd.read_csv('../data/crypto/1h.csv', index_col=0, parse_dates=True)\n",
    "context = pd.read_csv('../data/market_context/1h.csv', index_col=0, parse_dates=True)\n",
    "crypto_daily = pd.read_csv('../data/crypto/1d.csv', index_col=0, parse_dates=True)\n",
    "context_daily = pd.read_csv('../data/market_context/1d.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "# --- Align context to crypto (shift context back 30min to match crypto timestamps) ---\n",
    "context.index = context.index - pd.Timedelta(minutes=30)\n",
    "\n",
    "# --- Find common date range ---\n",
    "start_date = max(crypto.index.min(), context.index.min())\n",
    "end_date = min(crypto.index.max(), context.index.max())\n",
    "crypto = crypto[(crypto.index >= start_date) & (crypto.index <= end_date)]\n",
    "context = context[(context.index >= start_date) & (context.index <= end_date)]\n",
    "\n",
    "# --- Join and sort ---\n",
    "features = crypto.join(context, how='outer')\n",
    "features = features.sort_index().ffill()\n",
    "\n",
    "# drop cols that are all nan\n",
    "print(\"Before dropping all-nan columns:\", features.shape)\n",
    "features = features.dropna(axis=1, how='all')\n",
    "print(\"after dropping all-nan columns:\", features.shape)\n",
    "features = features.ffill().dropna()\n",
    "print(\"after dropping all-nan rows:\", features.shape)\n",
    "print('start date:', features.index.min(), 'end date:', features.index.max())\n",
    "display(features.tail(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4822145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features normalized. Shape: (12509, 1680)\n",
      "Targets shape: (12509, 6)\n"
     ]
    }
   ],
   "source": [
    "# --- Multi-horizon, multi-class targets (6 horizons, 7 classes) ---\n",
    "def make_targets(prices, horizons=[24, 48, 72, 96, 144, 240], thresholds=[-0.07, -0.03, 0, 0.03, 0.07]):\n",
    "    # thresholds: [-7%, -3%, -.1%, +.1%, +3%, +7%]\n",
    "    # classes: 0: < -7%, 1: -7% to -3%, 2: -3% to -1%, 3: -1% to +1%, 4: +1% to +3%, 5: +3% to +7%, 6: > +7%\n",
    "    targets = np.zeros((len(prices), len(horizons)), dtype=np.int64)\n",
    "    for h_idx, h in enumerate(horizons):\n",
    "        future = prices.shift(-h)\n",
    "        pct = (future - prices) / prices\n",
    "        targets[:, h_idx] = np.select([\n",
    "            pct <= thresholds[0],\n",
    "            (pct > thresholds[0]) & (pct <= thresholds[1]),\n",
    "            (pct > thresholds[1]) & (pct <= thresholds[2]),\n",
    "            (pct > thresholds[2]) & (pct <= thresholds[3]),\n",
    "            (pct > thresholds[3]) & (pct <= thresholds[4]),\n",
    "            (pct > thresholds[4])\n",
    "        ], [0, 1, 2, 3, 4, 5], default=3)\n",
    "    return targets\n",
    "\n",
    "horizons = [24, 48, 72, 96, 144, 240] # Num hours ahead (1 day, 2 days, 3 days, 4 days, 6 days, 10 days)\n",
    "targets = make_targets(features[TARGET_COL], horizons=horizons)\n",
    "\n",
    "# --- Drop rows with NaN targets (due to shifting) ---\n",
    "valid_idx = ~np.isnan(targets).any(axis=1)\n",
    "features = features.iloc[valid_idx]\n",
    "targets = targets[valid_idx]\n",
    "\n",
    "# --- Normalize features ---\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "print(\"Features normalized. Shape:\", features.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze class distribution for 1-day horizon (first target column) ---\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "one_day_targets = targets[:, 0]\n",
    "class_counts = Counter(one_day_targets)\n",
    "total = len(one_day_targets)\n",
    "print(\"Class distribution (1-day horizon):\")\n",
    "for cls, count in sorted(class_counts.items()):\n",
    "    print(f\"Class {cls}: {count} ({100*count/total:.2f}%)\")\n",
    "plt.bar(class_counts.keys(), [class_counts[k] for k in class_counts])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution (1-day horizon)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e40ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute class weights for 1-day horizon (for use in loss functions) ---\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(one_day_targets)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=one_day_targets)\n",
    "print(\"Class weights (1-day horizon):\", dict(zip(classes, class_weights)))\n",
    "\n",
    "# Example: For transformer, you can use these weights in nn.CrossEntropyLoss\n",
    "import torch\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "# Use criterion in your training loop for the 1-day horizon if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Drop the least important 10% of features (by previous RF importances) ---\n",
    "# Assumes 'importances' and 'feature_names' are available from previous RF run\n",
    "if 'importances' in globals() and importances is not None:\n",
    "    n_drop = max(1, int(0.1 * len(importances)))\n",
    "    drop_indices = np.argsort(importances)[:n_drop]\n",
    "    drop_features = [feature_names[i] for i in drop_indices]\n",
    "    print(f\"Dropping {n_drop} least important features:\", drop_features)\n",
    "    # If features is a DataFrame, drop columns; if ndarray, drop by index\n",
    "    if hasattr(features, 'columns'):\n",
    "        features = features.drop(columns=drop_features)\n",
    "    else:\n",
    "        features = np.delete(features, drop_indices, axis=1)\n",
    "else:\n",
    "    print(\"Feature importances not found. Skipping feature drop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Retrain Random Forest after feature pruning ---\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = features\n",
    "# Use the 1-day horizon (first column of targets) for classification\n",
    "y = targets[:, 0]\n",
    "\n",
    "# Split into train/val/test (same as before)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Retrain Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=SEED, n_jobs=-1, class_weight='balanced', oob_score=True)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "for split, X_split, y_split in zip(['Train', 'Val', 'Test'], [X_train, X_val, X_test], [y_train, y_val, y_test]):\n",
    "    y_pred = rf.predict(X_split)\n",
    "    print(f\"\\n{split} Classification Report:\")\n",
    "    print(classification_report(y_split, y_pred))\n",
    "    cm = confusion_matrix(y_split, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f'Random Forest Confusion Matrix - {split}')\n",
    "    plt.show()\n",
    "\n",
    "# Feature importances (after pruning)\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = features.columns if hasattr(features, 'columns') else [f'f{i}' for i in range(X.shape[1])]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Feature Importances (Top 20, after pruning)')\n",
    "plt.bar(range(min(20, len(importances))), importances[indices[:20]], align='center')\n",
    "plt.xticks(range(min(20, len(importances))), [feature_names[i] for i in indices[:20]], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show least important features (bottom 20)\n",
    "least_important = [(feature_names[i], importances[i]) for i in indices[-20:]]\n",
    "print(\"\\nLeast important features (bottom 20, after pruning):\")\n",
    "for name, imp in least_important:\n",
    "    print(f\"{name}: {imp:.6f}\")\n",
    "\n",
    "# Show confidence (max probability) for test set\n",
    "if hasattr(rf, 'predict_proba'):\n",
    "    proba = rf.predict_proba(X_test)\n",
    "    confidence = np.max(proba, axis=1)\n",
    "    print(\"\\nSample prediction confidences (test set):\")\n",
    "    print(confidence[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "45bd21cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 8588, Val samples: 1840, Test samples: 1841\n",
      "Feature shape: (12509, 1680), Targets shape: (12509, 6)\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare sequences for transformer ---\n",
    "SEQ_LEN = 240  # 10 days of hourly data (adjust as needed)\n",
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, features, targets, seq_len):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.seq_len\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx:idx+self.seq_len]\n",
    "        y = self.targets[idx+self.seq_len-1]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = CryptoDataset(features, targets, SEQ_LEN)\n",
    "total = len(dataset)\n",
    "train_end = int(0.7 * total)\n",
    "val_end = int(0.85 * total)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, range(0, train_end))\n",
    "val_dataset = torch.utils.data.Subset(dataset, range(train_end, val_end))\n",
    "test_dataset = torch.utils.data.Subset(dataset, range(val_end, total))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "print(f\"Feature shape: {features.shape}, Targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eeeb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b42022e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeltaSenseTransformer(\n",
      "  (input_proj): Linear(in_features=1680, out_features=64, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=64, out_features=36, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_dim = features.shape[1]\n",
    "seq_len = SEQ_LEN\n",
    "n_horizons = 6\n",
    "n_classes = 6\n",
    "\n",
    "model = DeltaSenseTransformer(\n",
    "    input_dim=input_dim,\n",
    "    seq_len=seq_len,\n",
    "    n_horizons=n_horizons,\n",
    "    n_classes=n_classes,\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_layers=3,\n",
    "    dim_feedforward=128,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "272633b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def hyperbolic_discount_loss(outputs, targets, K=0.05, horizons=None):\n",
    "    \"\"\"\n",
    "    outputs: (batch, n_horizons, n_classes) - logits\n",
    "    targets: (batch, n_horizons) - integer class labels\n",
    "    K: hyperbolic discount factor (Glimcher's K)\n",
    "    horizons: list/array of horizon steps (e.g. [24, 48, ...])\n",
    "    Penalizes by class distance and discounts by horizon.\n",
    "    \"\"\"\n",
    "    if horizons is None:\n",
    "        horizons = torch.arange(outputs.shape[1], device=outputs.device) + 1\n",
    "    batch, n_horizons, n_classes = outputs.shape\n",
    "    loss = 0.0\n",
    "    for h in range(n_horizons):\n",
    "        # Hyperbolic discount: 1/(1+K*t)\n",
    "        t = horizons[h] if hasattr(horizons, '__getitem__') else h+1\n",
    "        discount = 1.0 / (1.0 + K * t)\n",
    "        logits = outputs[:, h, :]\n",
    "        target = targets[:, h]\n",
    "        probs = F.log_softmax(logits, dim=-1)\n",
    "        # Penalize by class distance\n",
    "        penalty_matrix = torch.abs(torch.arange(n_classes, device=outputs.device).unsqueeze(0) - target.unsqueeze(1)).float()\n",
    "        # Cross-entropy for each class, weighted by distance\n",
    "        ce = -probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "        weighted_ce = (ce * 1.0 + (probs.exp() * penalty_matrix).sum(dim=1))\n",
    "        loss += discount * weighted_ce.mean()\n",
    "    return loss / n_horizons\n",
    "\n",
    "# Example usage in training loop:\n",
    "# loss = hyperbolic_discount_loss(out, yb, K=0.05, horizons=[24,48,72,96,144,240])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6514d548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.7007, val_loss=0.7563\n",
      "Epoch 2: train_loss=0.6861, val_loss=0.7499\n",
      "Epoch 2: train_loss=0.6861, val_loss=0.7499\n",
      "Epoch 3: train_loss=0.6834, val_loss=0.7490\n",
      "Epoch 3: train_loss=0.6834, val_loss=0.7490\n",
      "Epoch 4: train_loss=0.6819, val_loss=0.7493\n",
      "Epoch 4: train_loss=0.6819, val_loss=0.7493\n",
      "Epoch 5: train_loss=0.6781, val_loss=0.7594\n",
      "Epoch 5: train_loss=0.6781, val_loss=0.7594\n",
      "Epoch 6: train_loss=0.6755, val_loss=0.7522\n",
      "Epoch 6: train_loss=0.6755, val_loss=0.7522\n",
      "Epoch 7: train_loss=0.6776, val_loss=0.7386\n",
      "Epoch 7: train_loss=0.6776, val_loss=0.7386\n",
      "Epoch 8: train_loss=0.6608, val_loss=0.7533\n",
      "Epoch 8: train_loss=0.6608, val_loss=0.7533\n",
      "Epoch 9: train_loss=0.6647, val_loss=0.7775\n",
      "Epoch 9: train_loss=0.6647, val_loss=0.7775\n",
      "Epoch 10: train_loss=0.6787, val_loss=0.7803\n",
      "Epoch 10: train_loss=0.6787, val_loss=0.7803\n",
      "Epoch 11: train_loss=0.6640, val_loss=0.7963\n",
      "Epoch 11: train_loss=0.6640, val_loss=0.7963\n",
      "Epoch 12: train_loss=0.6579, val_loss=0.7827\n",
      "Epoch 12: train_loss=0.6579, val_loss=0.7827\n",
      "Epoch 13: train_loss=0.6599, val_loss=0.7696\n",
      "Epoch 13: train_loss=0.6599, val_loss=0.7696\n",
      "Epoch 14: train_loss=0.6520, val_loss=0.7417\n",
      "Epoch 14: train_loss=0.6520, val_loss=0.7417\n",
      "Epoch 15: train_loss=0.6579, val_loss=0.7926\n",
      "Epoch 15: train_loss=0.6579, val_loss=0.7926\n",
      "Epoch 16: train_loss=0.6642, val_loss=0.7746\n",
      "Epoch 16: train_loss=0.6642, val_loss=0.7746\n",
      "Epoch 17: train_loss=0.6463, val_loss=0.7960\n",
      "Epoch 17: train_loss=0.6463, val_loss=0.7960\n",
      "Epoch 18: train_loss=0.6406, val_loss=0.7730\n",
      "Epoch 18: train_loss=0.6406, val_loss=0.7730\n",
      "Epoch 19: train_loss=0.6356, val_loss=0.7350\n",
      "Epoch 19: train_loss=0.6356, val_loss=0.7350\n",
      "Epoch 20: train_loss=0.6425, val_loss=0.8116\n",
      "Epoch 20: train_loss=0.6425, val_loss=0.8116\n",
      "Epoch 21: train_loss=0.6394, val_loss=0.7881\n",
      "Epoch 21: train_loss=0.6394, val_loss=0.7881\n",
      "Epoch 22: train_loss=0.6175, val_loss=0.8424\n",
      "Epoch 22: train_loss=0.6175, val_loss=0.8424\n",
      "Epoch 23: train_loss=0.6253, val_loss=0.7531\n",
      "Epoch 23: train_loss=0.6253, val_loss=0.7531\n",
      "Epoch 24: train_loss=0.6196, val_loss=0.7118\n",
      "Epoch 24: train_loss=0.6196, val_loss=0.7118\n",
      "Epoch 25: train_loss=0.6035, val_loss=0.6855\n",
      "Epoch 25: train_loss=0.6035, val_loss=0.6855\n",
      "Epoch 26: train_loss=0.6105, val_loss=0.7188\n",
      "Epoch 26: train_loss=0.6105, val_loss=0.7188\n",
      "Epoch 27: train_loss=0.6001, val_loss=0.7262\n",
      "Epoch 27: train_loss=0.6001, val_loss=0.7262\n",
      "Epoch 28: train_loss=0.6002, val_loss=0.6776\n",
      "Epoch 28: train_loss=0.6002, val_loss=0.6776\n",
      "Epoch 29: train_loss=0.5953, val_loss=0.7168\n",
      "Epoch 29: train_loss=0.5953, val_loss=0.7168\n",
      "Epoch 30: train_loss=0.5907, val_loss=0.7713\n",
      "Epoch 30: train_loss=0.5907, val_loss=0.7713\n",
      "Epoch 31: train_loss=0.5864, val_loss=0.7569\n",
      "Epoch 31: train_loss=0.5864, val_loss=0.7569\n",
      "Epoch 32: train_loss=0.6033, val_loss=0.7478\n",
      "Epoch 32: train_loss=0.6033, val_loss=0.7478\n",
      "Epoch 33: train_loss=0.5914, val_loss=0.7569\n",
      "Epoch 33: train_loss=0.5914, val_loss=0.7569\n",
      "Epoch 34: train_loss=0.5750, val_loss=0.7509\n",
      "Epoch 34: train_loss=0.5750, val_loss=0.7509\n",
      "Epoch 35: train_loss=0.5638, val_loss=0.7464\n",
      "Epoch 35: train_loss=0.5638, val_loss=0.7464\n",
      "Epoch 36: train_loss=0.5774, val_loss=0.8714\n",
      "Epoch 36: train_loss=0.5774, val_loss=0.8714\n",
      "Epoch 37: train_loss=0.5685, val_loss=0.9022\n",
      "Epoch 37: train_loss=0.5685, val_loss=0.9022\n",
      "Epoch 38: train_loss=0.5614, val_loss=0.7420\n",
      "Epoch 38: train_loss=0.5614, val_loss=0.7420\n",
      "Epoch 39: train_loss=0.5818, val_loss=0.9604\n",
      "Epoch 39: train_loss=0.5818, val_loss=0.9604\n",
      "Epoch 40: train_loss=0.5590, val_loss=0.9275\n",
      "Epoch 40: train_loss=0.5590, val_loss=0.9275\n",
      "Epoch 41: train_loss=0.5485, val_loss=0.7605\n",
      "Epoch 41: train_loss=0.5485, val_loss=0.7605\n",
      "Epoch 42: train_loss=0.5513, val_loss=0.7766\n",
      "Epoch 42: train_loss=0.5513, val_loss=0.7766\n",
      "Epoch 43: train_loss=0.5511, val_loss=0.7247\n",
      "Epoch 43: train_loss=0.5511, val_loss=0.7247\n",
      "Epoch 44: train_loss=0.5605, val_loss=0.8480\n",
      "Epoch 44: train_loss=0.5605, val_loss=0.8480\n",
      "Epoch 45: train_loss=0.5309, val_loss=0.9047\n",
      "Epoch 45: train_loss=0.5309, val_loss=0.9047\n",
      "Epoch 46: train_loss=0.5186, val_loss=0.8522\n",
      "Epoch 46: train_loss=0.5186, val_loss=0.8522\n",
      "Epoch 47: train_loss=0.5196, val_loss=0.8695\n",
      "Epoch 47: train_loss=0.5196, val_loss=0.8695\n",
      "Epoch 48: train_loss=0.5082, val_loss=0.8980\n",
      "Epoch 48: train_loss=0.5082, val_loss=0.8980\n",
      "Epoch 49: train_loss=0.5100, val_loss=0.8896\n",
      "Epoch 49: train_loss=0.5100, val_loss=0.8896\n",
      "Epoch 50: train_loss=0.5111, val_loss=1.0428\n",
      "Epoch 50: train_loss=0.5111, val_loss=1.0428\n",
      "Epoch 51: train_loss=0.5112, val_loss=0.7986\n",
      "Epoch 51: train_loss=0.5112, val_loss=0.7986\n",
      "Epoch 52: train_loss=0.5119, val_loss=0.9458\n",
      "Epoch 52: train_loss=0.5119, val_loss=0.9458\n",
      "Epoch 53: train_loss=0.5033, val_loss=0.8656\n",
      "Epoch 53: train_loss=0.5033, val_loss=0.8656\n",
      "Epoch 54: train_loss=0.4972, val_loss=0.8611\n",
      "Epoch 54: train_loss=0.4972, val_loss=0.8611\n",
      "Epoch 55: train_loss=0.5116, val_loss=0.8172\n",
      "Epoch 55: train_loss=0.5116, val_loss=0.8172\n",
      "Epoch 56: train_loss=0.5178, val_loss=0.7760\n",
      "Epoch 56: train_loss=0.5178, val_loss=0.7760\n",
      "Epoch 57: train_loss=0.4925, val_loss=0.8483\n",
      "Epoch 57: train_loss=0.4925, val_loss=0.8483\n",
      "Epoch 58: train_loss=0.4784, val_loss=0.9178\n",
      "Epoch 58: train_loss=0.4784, val_loss=0.9178\n",
      "Epoch 59: train_loss=0.4706, val_loss=0.8665\n",
      "Epoch 59: train_loss=0.4706, val_loss=0.8665\n",
      "Epoch 60: train_loss=0.4685, val_loss=0.8752\n",
      "Epoch 60: train_loss=0.4685, val_loss=0.8752\n",
      "Epoch 61: train_loss=0.4684, val_loss=0.8400\n",
      "Epoch 61: train_loss=0.4684, val_loss=0.8400\n",
      "Epoch 62: train_loss=0.4699, val_loss=0.9172\n",
      "Epoch 62: train_loss=0.4699, val_loss=0.9172\n",
      "Epoch 63: train_loss=0.4588, val_loss=0.9562\n",
      "Epoch 63: train_loss=0.4588, val_loss=0.9562\n",
      "Epoch 64: train_loss=0.4625, val_loss=0.8877\n",
      "Epoch 64: train_loss=0.4625, val_loss=0.8877\n",
      "Epoch 65: train_loss=0.4471, val_loss=1.0809\n",
      "Epoch 65: train_loss=0.4471, val_loss=1.0809\n",
      "Epoch 66: train_loss=0.4474, val_loss=0.7709\n",
      "Epoch 66: train_loss=0.4474, val_loss=0.7709\n",
      "Epoch 67: train_loss=0.4479, val_loss=0.9369\n",
      "Epoch 67: train_loss=0.4479, val_loss=0.9369\n",
      "Epoch 68: train_loss=0.4488, val_loss=1.0592\n",
      "Epoch 68: train_loss=0.4488, val_loss=1.0592\n",
      "Epoch 69: train_loss=0.4866, val_loss=0.8013\n",
      "Epoch 69: train_loss=0.4866, val_loss=0.8013\n",
      "Epoch 70: train_loss=0.4627, val_loss=1.0667\n",
      "Epoch 70: train_loss=0.4627, val_loss=1.0667\n",
      "Epoch 71: train_loss=0.4650, val_loss=0.9151\n",
      "Epoch 71: train_loss=0.4650, val_loss=0.9151\n",
      "Epoch 72: train_loss=0.4346, val_loss=0.9252\n",
      "Epoch 72: train_loss=0.4346, val_loss=0.9252\n",
      "Epoch 73: train_loss=0.4235, val_loss=0.9512\n",
      "Epoch 73: train_loss=0.4235, val_loss=0.9512\n",
      "Epoch 74: train_loss=0.4289, val_loss=0.8784\n",
      "Epoch 74: train_loss=0.4289, val_loss=0.8784\n",
      "Epoch 75: train_loss=0.4415, val_loss=0.8863\n",
      "Epoch 75: train_loss=0.4415, val_loss=0.8863\n",
      "Epoch 76: train_loss=0.4219, val_loss=1.0684\n",
      "Epoch 76: train_loss=0.4219, val_loss=1.0684\n",
      "Epoch 77: train_loss=0.4127, val_loss=0.9384\n",
      "Epoch 77: train_loss=0.4127, val_loss=0.9384\n",
      "Epoch 78: train_loss=0.4119, val_loss=0.9102\n",
      "Epoch 78: train_loss=0.4119, val_loss=0.9102\n",
      "Epoch 79: train_loss=0.4389, val_loss=0.8777\n",
      "Epoch 79: train_loss=0.4389, val_loss=0.8777\n",
      "Epoch 80: train_loss=0.4177, val_loss=0.9855\n",
      "Epoch 80: train_loss=0.4177, val_loss=0.9855\n",
      "Epoch 81: train_loss=0.4107, val_loss=1.0910\n",
      "Epoch 81: train_loss=0.4107, val_loss=1.0910\n",
      "Epoch 82: train_loss=0.4130, val_loss=1.1480\n",
      "Epoch 82: train_loss=0.4130, val_loss=1.1480\n",
      "Epoch 83: train_loss=0.4134, val_loss=0.9518\n",
      "Epoch 83: train_loss=0.4134, val_loss=0.9518\n",
      "Epoch 84: train_loss=0.4409, val_loss=0.8437\n",
      "Epoch 84: train_loss=0.4409, val_loss=0.8437\n",
      "Epoch 85: train_loss=0.4597, val_loss=0.8511\n",
      "Epoch 85: train_loss=0.4597, val_loss=0.8511\n",
      "Epoch 86: train_loss=0.4253, val_loss=0.8996\n",
      "Epoch 86: train_loss=0.4253, val_loss=0.8996\n",
      "Epoch 87: train_loss=0.4107, val_loss=0.9956\n",
      "Epoch 87: train_loss=0.4107, val_loss=0.9956\n",
      "Epoch 88: train_loss=0.4129, val_loss=0.8777\n",
      "Epoch 88: train_loss=0.4129, val_loss=0.8777\n",
      "Epoch 89: train_loss=0.4090, val_loss=0.9256\n",
      "Epoch 89: train_loss=0.4090, val_loss=0.9256\n",
      "Epoch 90: train_loss=0.3968, val_loss=0.8581\n",
      "Epoch 90: train_loss=0.3968, val_loss=0.8581\n",
      "Epoch 91: train_loss=0.3918, val_loss=0.9143\n",
      "Epoch 91: train_loss=0.3918, val_loss=0.9143\n",
      "Epoch 92: train_loss=0.3832, val_loss=0.9402\n",
      "Epoch 92: train_loss=0.3832, val_loss=0.9402\n",
      "Epoch 93: train_loss=0.3822, val_loss=0.9092\n",
      "Epoch 93: train_loss=0.3822, val_loss=0.9092\n",
      "Epoch 94: train_loss=0.3822, val_loss=0.8969\n",
      "Epoch 94: train_loss=0.3822, val_loss=0.8969\n",
      "Epoch 95: train_loss=0.3856, val_loss=0.9936\n",
      "Epoch 95: train_loss=0.3856, val_loss=0.9936\n",
      "Epoch 96: train_loss=0.4007, val_loss=0.9098\n",
      "Epoch 96: train_loss=0.4007, val_loss=0.9098\n",
      "Epoch 97: train_loss=0.4009, val_loss=0.9737\n",
      "Epoch 97: train_loss=0.4009, val_loss=0.9737\n",
      "Epoch 98: train_loss=0.3841, val_loss=0.9330\n",
      "Epoch 98: train_loss=0.3841, val_loss=0.9330\n",
      "Epoch 99: train_loss=0.3768, val_loss=0.8915\n",
      "Epoch 99: train_loss=0.3768, val_loss=0.8915\n",
      "Epoch 100: train_loss=0.3760, val_loss=0.9360\n",
      "Epoch 100: train_loss=0.3760, val_loss=0.9360\n",
      "Epoch 101: train_loss=0.3731, val_loss=1.0125\n",
      "Epoch 101: train_loss=0.3731, val_loss=1.0125\n",
      "Epoch 102: train_loss=0.3757, val_loss=0.9217\n",
      "Epoch 102: train_loss=0.3757, val_loss=0.9217\n",
      "Epoch 103: train_loss=0.3694, val_loss=0.8774\n",
      "Epoch 103: train_loss=0.3694, val_loss=0.8774\n",
      "Epoch 104: train_loss=0.3732, val_loss=0.9430\n",
      "Epoch 104: train_loss=0.3732, val_loss=0.9430\n",
      "Epoch 105: train_loss=0.3630, val_loss=0.9730\n",
      "Epoch 105: train_loss=0.3630, val_loss=0.9730\n",
      "Epoch 106: train_loss=0.3582, val_loss=1.0098\n",
      "Epoch 106: train_loss=0.3582, val_loss=1.0098\n",
      "Epoch 107: train_loss=0.3656, val_loss=0.9256\n",
      "Epoch 107: train_loss=0.3656, val_loss=0.9256\n",
      "Epoch 108: train_loss=0.3643, val_loss=0.9332\n",
      "Epoch 108: train_loss=0.3643, val_loss=0.9332\n",
      "Epoch 109: train_loss=0.3592, val_loss=1.0255\n",
      "Epoch 109: train_loss=0.3592, val_loss=1.0255\n",
      "Epoch 110: train_loss=0.3514, val_loss=0.9803\n",
      "Epoch 110: train_loss=0.3514, val_loss=0.9803\n",
      "Epoch 111: train_loss=0.3447, val_loss=0.9119\n",
      "Epoch 111: train_loss=0.3447, val_loss=0.9119\n",
      "Epoch 112: train_loss=0.3334, val_loss=0.9748\n",
      "Epoch 112: train_loss=0.3334, val_loss=0.9748\n",
      "Epoch 113: train_loss=0.3498, val_loss=0.9500\n",
      "Epoch 113: train_loss=0.3498, val_loss=0.9500\n",
      "Epoch 114: train_loss=0.3540, val_loss=0.9730\n",
      "Epoch 114: train_loss=0.3540, val_loss=0.9730\n",
      "Epoch 115: train_loss=0.3417, val_loss=1.0289\n",
      "Epoch 115: train_loss=0.3417, val_loss=1.0289\n",
      "Epoch 116: train_loss=0.3416, val_loss=0.9993\n",
      "Epoch 116: train_loss=0.3416, val_loss=0.9993\n",
      "Epoch 117: train_loss=0.3447, val_loss=1.0176\n",
      "Epoch 117: train_loss=0.3447, val_loss=1.0176\n",
      "Epoch 118: train_loss=0.3450, val_loss=1.0177\n",
      "Epoch 118: train_loss=0.3450, val_loss=1.0177\n",
      "Epoch 119: train_loss=0.3594, val_loss=0.9999\n",
      "Epoch 119: train_loss=0.3594, val_loss=0.9999\n",
      "Epoch 120: train_loss=0.3655, val_loss=0.9510\n",
      "Epoch 120: train_loss=0.3655, val_loss=0.9510\n",
      "Epoch 121: train_loss=0.3550, val_loss=0.9288\n",
      "Epoch 121: train_loss=0.3550, val_loss=0.9288\n",
      "Epoch 122: train_loss=0.3440, val_loss=0.9197\n",
      "Epoch 122: train_loss=0.3440, val_loss=0.9197\n",
      "Epoch 123: train_loss=0.3327, val_loss=0.9999\n",
      "Epoch 123: train_loss=0.3327, val_loss=0.9999\n",
      "Epoch 124: train_loss=0.3231, val_loss=0.9796\n",
      "Epoch 124: train_loss=0.3231, val_loss=0.9796\n",
      "Epoch 125: train_loss=0.3223, val_loss=0.9419\n",
      "Epoch 125: train_loss=0.3223, val_loss=0.9419\n",
      "Epoch 126: train_loss=0.3186, val_loss=0.9843\n",
      "Epoch 126: train_loss=0.3186, val_loss=0.9843\n",
      "Epoch 127: train_loss=0.3253, val_loss=0.9369\n",
      "Epoch 127: train_loss=0.3253, val_loss=0.9369\n",
      "Epoch 128: train_loss=0.3267, val_loss=1.0222\n",
      "Epoch 128: train_loss=0.3267, val_loss=1.0222\n",
      "Epoch 129: train_loss=0.3306, val_loss=1.0220\n",
      "Epoch 129: train_loss=0.3306, val_loss=1.0220\n",
      "Epoch 130: train_loss=0.3354, val_loss=1.0310\n",
      "Epoch 130: train_loss=0.3354, val_loss=1.0310\n",
      "Epoch 131: train_loss=0.3244, val_loss=1.0169\n",
      "Epoch 131: train_loss=0.3244, val_loss=1.0169\n",
      "Epoch 132: train_loss=0.3231, val_loss=0.9782\n",
      "Epoch 132: train_loss=0.3231, val_loss=0.9782\n",
      "Epoch 133: train_loss=0.3199, val_loss=1.0960\n",
      "Epoch 133: train_loss=0.3199, val_loss=1.0960\n",
      "Epoch 134: train_loss=0.3097, val_loss=1.0169\n",
      "Epoch 134: train_loss=0.3097, val_loss=1.0169\n",
      "Epoch 135: train_loss=0.3055, val_loss=1.0363\n",
      "Epoch 135: train_loss=0.3055, val_loss=1.0363\n",
      "Epoch 136: train_loss=0.3028, val_loss=1.0362\n",
      "Epoch 136: train_loss=0.3028, val_loss=1.0362\n",
      "Epoch 137: train_loss=0.3060, val_loss=1.0552\n",
      "Epoch 137: train_loss=0.3060, val_loss=1.0552\n",
      "Epoch 138: train_loss=0.3103, val_loss=1.0035\n",
      "Epoch 138: train_loss=0.3103, val_loss=1.0035\n",
      "Epoch 139: train_loss=0.3175, val_loss=0.9741\n",
      "Epoch 139: train_loss=0.3175, val_loss=0.9741\n",
      "Epoch 140: train_loss=0.3075, val_loss=1.0048\n",
      "Epoch 140: train_loss=0.3075, val_loss=1.0048\n",
      "Epoch 141: train_loss=0.3040, val_loss=1.0257\n",
      "Epoch 141: train_loss=0.3040, val_loss=1.0257\n",
      "Epoch 142: train_loss=0.3164, val_loss=1.0206\n",
      "Epoch 142: train_loss=0.3164, val_loss=1.0206\n",
      "Epoch 143: train_loss=0.3106, val_loss=0.9552\n",
      "Epoch 143: train_loss=0.3106, val_loss=0.9552\n",
      "Epoch 144: train_loss=0.3132, val_loss=1.0727\n",
      "Epoch 144: train_loss=0.3132, val_loss=1.0727\n",
      "Epoch 145: train_loss=0.3145, val_loss=1.0222\n",
      "Epoch 145: train_loss=0.3145, val_loss=1.0222\n",
      "Epoch 146: train_loss=0.3065, val_loss=1.0346\n",
      "Epoch 146: train_loss=0.3065, val_loss=1.0346\n",
      "Epoch 147: train_loss=0.3067, val_loss=0.9862\n",
      "Epoch 147: train_loss=0.3067, val_loss=0.9862\n",
      "Epoch 148: train_loss=0.2969, val_loss=1.0051\n",
      "Epoch 148: train_loss=0.2969, val_loss=1.0051\n",
      "Epoch 149: train_loss=0.2962, val_loss=1.0674\n",
      "Epoch 149: train_loss=0.2962, val_loss=1.0674\n",
      "Epoch 150: train_loss=0.2994, val_loss=1.0069\n",
      "Epoch 150: train_loss=0.2994, val_loss=1.0069\n",
      "Epoch 151: train_loss=0.3025, val_loss=0.9916\n",
      "Early stopping at epoch 151\n",
      "Epoch 151: train_loss=0.3025, val_loss=0.9916\n",
      "Early stopping at epoch 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saris\\AppData\\Local\\Temp\\ipykernel_11328\\3395653999.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "# --- Training loop with early stopping ---\n",
    "EPOCHS = 300\n",
    "PATIENCE = 5\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "train_losses, val_losses = [], []\n",
    "best_model_path = f'../notebooks/best_{TARGET_COL}_transformer.pth'\n",
    "\n",
    "# Use the custom hyperbolic discounting loss\n",
    "def get_horizons():\n",
    "    return [24, 48, 72, 96, 144, 240]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = hyperbolic_discount_loss(out, yb, K=0.05, horizons=get_horizons())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            out = model(xb)\n",
    "            loss = hyperbolic_discount_loss(out, yb, K=0.05, horizons=get_horizons())\n",
    "            val_running_loss += loss.item() * xb.size(0)\n",
    "    val_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= PATIENCE and epoch >= 150:  # Start checking after 10 epochs\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# --- Plot train/val losses ---\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plt.plot(train_losses, label='Train Loss')\n",
    "# plt.plot(val_losses, label='Val Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.show()\n",
    "\n",
    "# --- Confusion matrix for each horizon ---\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        out = model(xb)  # (batch, n_horizons, n_classes)\n",
    "        preds = out.argmax(-1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(yb.numpy())\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# class_names = ['<-7%', '-7%~-3%', '-3%~-1%', '-1%~+1%', '+1%~+3%', '+3%~+7%', '>+7%']\n",
    "# for h in range(n_horizons):\n",
    "#     cm = confusion_matrix(all_targets[:,h], all_preds[:,h], labels=list(range(7)))\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "#     disp.plot(cmap='Blues')\n",
    "#     plt.title(f'Confusion Matrix - Horizon {h+1}')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfc610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
