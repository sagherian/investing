{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6b3f208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "Cuda version: 11.8\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and set up config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import random\n",
    "# Data pipeline: Load, join, align, feature engineering, targets, normalization, and sequence prep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from model_def import DeltaSenseTransformer\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Torch version:', torch.__version__)\n",
    "print('Cuda version:', torch.version.cuda)\n",
    "print('Using device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42332b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0628a02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping all-nan columns: (16493, 1742)\n",
      "after dropping all-nan columns: (16493, 1680)\n",
      "after dropping all-nan rows: (12509, 1680)\n",
      "start date: 2024-01-27 13:00:00+00:00 end date: 2025-07-01 17:00:00+00:00\n",
      "after dropping all-nan rows: (12509, 1680)\n",
      "start date: 2024-01-27 13:00:00+00:00 end date: 2025-07-01 17:00:00+00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1h_BTC-USD_Close</th>\n",
       "      <th>1h_BTC-USD_High</th>\n",
       "      <th>1h_BTC-USD_Low</th>\n",
       "      <th>1h_BTC-USD_Open</th>\n",
       "      <th>1h_BTC-USD_Volume</th>\n",
       "      <th>1h_BTC-USD_Pct_Change_1</th>\n",
       "      <th>1h_BTC-USD_Pct_Change_5</th>\n",
       "      <th>1h_BTC-USD_Pct_Change_10</th>\n",
       "      <th>1h_BTC-USD_SMA_10</th>\n",
       "      <th>1h_BTC-USD_SMA_20</th>\n",
       "      <th>...</th>\n",
       "      <th>1h_VXZ_BB_upper</th>\n",
       "      <th>1h_VXZ_BB_lower</th>\n",
       "      <th>1h_VXZ_RSI</th>\n",
       "      <th>1h_VXZ_OBV</th>\n",
       "      <th>1h_VXZ_ATR</th>\n",
       "      <th>1h_VXZ_MFI</th>\n",
       "      <th>1h_VXZ_Hist_Volatility</th>\n",
       "      <th>1h_VXZ_Donchian_Upper</th>\n",
       "      <th>1h_VXZ_Donchian_Lower</th>\n",
       "      <th>1h_VXZ_Z_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-07-01 16:00:00+00:00</th>\n",
       "      <td>106075.531250</td>\n",
       "      <td>106282.726562</td>\n",
       "      <td>105926.562500</td>\n",
       "      <td>106027.320312</td>\n",
       "      <td>1.142259e+10</td>\n",
       "      <td>0.029940</td>\n",
       "      <td>-0.502636</td>\n",
       "      <td>-0.787414</td>\n",
       "      <td>106495.921875</td>\n",
       "      <td>106812.092969</td>\n",
       "      <td>...</td>\n",
       "      <td>58.537098</td>\n",
       "      <td>57.645352</td>\n",
       "      <td>67.354319</td>\n",
       "      <td>-1413108.0</td>\n",
       "      <td>0.152536</td>\n",
       "      <td>71.730843</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>58.43</td>\n",
       "      <td>57.75</td>\n",
       "      <td>0.824345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-01 17:00:00+00:00</th>\n",
       "      <td>106219.570312</td>\n",
       "      <td>106315.867188</td>\n",
       "      <td>105958.820312</td>\n",
       "      <td>106114.359375</td>\n",
       "      <td>6.050824e+09</td>\n",
       "      <td>0.135789</td>\n",
       "      <td>-0.228747</td>\n",
       "      <td>-0.895195</td>\n",
       "      <td>106399.975781</td>\n",
       "      <td>106764.118750</td>\n",
       "      <td>...</td>\n",
       "      <td>58.537098</td>\n",
       "      <td>57.645352</td>\n",
       "      <td>67.354319</td>\n",
       "      <td>-1413108.0</td>\n",
       "      <td>0.152536</td>\n",
       "      <td>71.730843</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>58.43</td>\n",
       "      <td>57.75</td>\n",
       "      <td>0.824345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 1680 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           1h_BTC-USD_Close  1h_BTC-USD_High  1h_BTC-USD_Low  \\\n",
       "2025-07-01 16:00:00+00:00     106075.531250    106282.726562   105926.562500   \n",
       "2025-07-01 17:00:00+00:00     106219.570312    106315.867188   105958.820312   \n",
       "\n",
       "                           1h_BTC-USD_Open  1h_BTC-USD_Volume  \\\n",
       "2025-07-01 16:00:00+00:00    106027.320312       1.142259e+10   \n",
       "2025-07-01 17:00:00+00:00    106114.359375       6.050824e+09   \n",
       "\n",
       "                           1h_BTC-USD_Pct_Change_1  1h_BTC-USD_Pct_Change_5  \\\n",
       "2025-07-01 16:00:00+00:00                 0.029940                -0.502636   \n",
       "2025-07-01 17:00:00+00:00                 0.135789                -0.228747   \n",
       "\n",
       "                           1h_BTC-USD_Pct_Change_10  1h_BTC-USD_SMA_10  \\\n",
       "2025-07-01 16:00:00+00:00                 -0.787414      106495.921875   \n",
       "2025-07-01 17:00:00+00:00                 -0.895195      106399.975781   \n",
       "\n",
       "                           1h_BTC-USD_SMA_20  ...  1h_VXZ_BB_upper  \\\n",
       "2025-07-01 16:00:00+00:00      106812.092969  ...        58.537098   \n",
       "2025-07-01 17:00:00+00:00      106764.118750  ...        58.537098   \n",
       "\n",
       "                           1h_VXZ_BB_lower  1h_VXZ_RSI  1h_VXZ_OBV  \\\n",
       "2025-07-01 16:00:00+00:00        57.645352   67.354319  -1413108.0   \n",
       "2025-07-01 17:00:00+00:00        57.645352   67.354319  -1413108.0   \n",
       "\n",
       "                           1h_VXZ_ATR  1h_VXZ_MFI  1h_VXZ_Hist_Volatility  \\\n",
       "2025-07-01 16:00:00+00:00    0.152536   71.730843                0.007874   \n",
       "2025-07-01 17:00:00+00:00    0.152536   71.730843                0.007874   \n",
       "\n",
       "                           1h_VXZ_Donchian_Upper  1h_VXZ_Donchian_Lower  \\\n",
       "2025-07-01 16:00:00+00:00                  58.43                  57.75   \n",
       "2025-07-01 17:00:00+00:00                  58.43                  57.75   \n",
       "\n",
       "                           1h_VXZ_Z_Score  \n",
       "2025-07-01 16:00:00+00:00        0.824345  \n",
       "2025-07-01 17:00:00+00:00        0.824345  \n",
       "\n",
       "[2 rows x 1680 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TARGET_COL = '1h_ETH-USD_Close'\n",
    "\n",
    "crypto = pd.read_csv('../data/crypto/1h.csv', index_col=0, parse_dates=True)\n",
    "context = pd.read_csv('../data/market_context/1h.csv', index_col=0, parse_dates=True)\n",
    "crypto_daily = pd.read_csv('../data/crypto/1d.csv', index_col=0, parse_dates=True)\n",
    "context_daily = pd.read_csv('../data/market_context/1d.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "# --- Align context to crypto (shift context back 30min to match crypto timestamps) ---\n",
    "context.index = context.index - pd.Timedelta(minutes=30)\n",
    "\n",
    "# --- Find common date range ---\n",
    "start_date = max(crypto.index.min(), context.index.min())\n",
    "end_date = min(crypto.index.max(), context.index.max())\n",
    "crypto = crypto[(crypto.index >= start_date) & (crypto.index <= end_date)]\n",
    "context = context[(context.index >= start_date) & (context.index <= end_date)]\n",
    "\n",
    "# --- Join and sort ---\n",
    "features = crypto.join(context, how='outer')\n",
    "features = features.sort_index().ffill()\n",
    "\n",
    "# drop cols that are all nan\n",
    "print(\"Before dropping all-nan columns:\", features.shape)\n",
    "features = features.dropna(axis=1, how='all')\n",
    "print(\"after dropping all-nan columns:\", features.shape)\n",
    "features = features.ffill().dropna()\n",
    "print(\"after dropping all-nan rows:\", features.shape)\n",
    "print('start date:', features.index.min(), 'end date:', features.index.max())\n",
    "display(features.tail(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4822145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features normalized. Shape: (12509, 1680)\n",
      "Targets shape: (12509, 6)\n"
     ]
    }
   ],
   "source": [
    "# --- Multi-horizon, multi-class targets (6 horizons, 7 classes) ---\n",
    "def make_targets(prices, horizons=[24, 48, 72, 96, 144, 240], thresholds=[-0.07, -0.03, 0, 0.03, 0.07]):\n",
    "    # thresholds: [-7%, -3%, -.1%, +.1%, +3%, +7%]\n",
    "    # classes: 0: < -7%, 1: -7% to -3%, 2: -3% to -1%, 3: -1% to +1%, 4: +1% to +3%, 5: +3% to +7%, 6: > +7%\n",
    "    targets = np.zeros((len(prices), len(horizons)), dtype=np.int64)\n",
    "    for h_idx, h in enumerate(horizons):\n",
    "        future = prices.shift(-h)\n",
    "        pct = (future - prices) / prices\n",
    "        targets[:, h_idx] = np.select([\n",
    "            pct <= thresholds[0],\n",
    "            (pct > thresholds[0]) & (pct <= thresholds[1]),\n",
    "            (pct > thresholds[1]) & (pct <= thresholds[2]),\n",
    "            (pct > thresholds[2]) & (pct <= thresholds[3]),\n",
    "            (pct > thresholds[3]) & (pct <= thresholds[4]),\n",
    "            (pct > thresholds[4])\n",
    "        ], [0, 1, 2, 3, 4, 5], default=3)\n",
    "    return targets\n",
    "\n",
    "horizons = [24, 48, 72, 96, 144, 240] # Num hours ahead (1 day, 2 days, 3 days, 4 days, 6 days, 10 days)\n",
    "targets = make_targets(features[TARGET_COL], horizons=horizons)\n",
    "\n",
    "# --- Drop rows with NaN targets (due to shifting) ---\n",
    "valid_idx = ~np.isnan(targets).any(axis=1)\n",
    "features = features.iloc[valid_idx]\n",
    "targets = targets[valid_idx]\n",
    "\n",
    "# --- Normalize features ---\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "print(\"Features normalized. Shape:\", features.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45bd21cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 8588, Val samples: 1840, Test samples: 1841\n",
      "Feature shape: (12509, 1680), Targets shape: (12509, 6)\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare sequences for transformer ---\n",
    "SEQ_LEN = 240  # 10 days of hourly data (adjust as needed)\n",
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, features, targets, seq_len):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.seq_len = seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.seq_len\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx:idx+self.seq_len]\n",
    "        y = self.targets[idx+self.seq_len-1]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = CryptoDataset(features, targets, SEQ_LEN)\n",
    "total = len(dataset)\n",
    "train_end = int(0.7 * total)\n",
    "val_end = int(0.85 * total)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, range(0, train_end))\n",
    "val_dataset = torch.utils.data.Subset(dataset, range(train_end, val_end))\n",
    "test_dataset = torch.utils.data.Subset(dataset, range(val_end, total))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "print(f\"Feature shape: {features.shape}, Targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eeeb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b42022e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeltaSenseTransformer(\n",
      "  (input_proj): Linear(in_features=1680, out_features=64, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=64, out_features=36, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_dim = features.shape[1]\n",
    "seq_len = SEQ_LEN\n",
    "n_horizons = 6\n",
    "n_classes = 6\n",
    "\n",
    "model = DeltaSenseTransformer(\n",
    "    input_dim=input_dim,\n",
    "    seq_len=seq_len,\n",
    "    n_horizons=n_horizons,\n",
    "    n_classes=n_classes,\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_layers=3,\n",
    "    dim_feedforward=128,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "272633b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def hyperbolic_discount_loss(outputs, targets, K=0.05, horizons=None):\n",
    "    \"\"\"\n",
    "    outputs: (batch, n_horizons, n_classes) - logits\n",
    "    targets: (batch, n_horizons) - integer class labels\n",
    "    K: hyperbolic discount factor (Glimcher's K)\n",
    "    horizons: list/array of horizon steps (e.g. [24, 48, ...])\n",
    "    Penalizes by class distance and discounts by horizon.\n",
    "    \"\"\"\n",
    "    if horizons is None:\n",
    "        horizons = torch.arange(outputs.shape[1], device=outputs.device) + 1\n",
    "    batch, n_horizons, n_classes = outputs.shape\n",
    "    loss = 0.0\n",
    "    for h in range(n_horizons):\n",
    "        # Hyperbolic discount: 1/(1+K*t)\n",
    "        t = horizons[h] if hasattr(horizons, '__getitem__') else h+1\n",
    "        discount = 1.0 / (1.0 + K * t)\n",
    "        logits = outputs[:, h, :]\n",
    "        target = targets[:, h]\n",
    "        probs = F.log_softmax(logits, dim=-1)\n",
    "        # Penalize by class distance\n",
    "        penalty_matrix = torch.abs(torch.arange(n_classes, device=outputs.device).unsqueeze(0) - target.unsqueeze(1)).float()\n",
    "        # Cross-entropy for each class, weighted by distance\n",
    "        ce = -probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "        weighted_ce = (ce * 1.0 + (probs.exp() * penalty_matrix).sum(dim=1))\n",
    "        loss += discount * weighted_ce.mean()\n",
    "    return loss / n_horizons\n",
    "\n",
    "# Example usage in training loop:\n",
    "# loss = hyperbolic_discount_loss(out, yb, K=0.05, horizons=[24,48,72,96,144,240])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514d548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.6153, val_loss=0.8514\n",
      "Epoch 2: train_loss=0.6318, val_loss=0.8362\n",
      "Epoch 3: train_loss=0.6270, val_loss=0.8002\n",
      "Epoch 4: train_loss=0.6169, val_loss=0.7754\n",
      "Epoch 5: train_loss=0.6104, val_loss=0.7600\n",
      "Epoch 6: train_loss=0.6087, val_loss=0.8348\n",
      "Epoch 7: train_loss=0.6072, val_loss=0.7429\n",
      "Epoch 8: train_loss=0.5980, val_loss=0.7534\n",
      "Epoch 9: train_loss=0.5934, val_loss=0.7950\n",
      "Epoch 10: train_loss=0.5955, val_loss=0.7625\n"
     ]
    }
   ],
   "source": [
    "# --- Training loop with early stopping ---\n",
    "EPOCHS = 300\n",
    "PATIENCE = 5\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "train_losses, val_losses = [], []\n",
    "best_model_path = f'../notebooks/best_{TARGET_COL}_transformer.pth'\n",
    "\n",
    "# Use the custom hyperbolic discounting loss\n",
    "def get_horizons():\n",
    "    return [24, 48, 72, 96, 144, 240]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = hyperbolic_discount_loss(out, yb, K=0.05, horizons=get_horizons())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            out = model(xb)\n",
    "            loss = hyperbolic_discount_loss(out, yb, K=0.05, horizons=get_horizons())\n",
    "            val_running_loss += loss.item() * xb.size(0)\n",
    "    val_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= PATIENCE and epoch >= 150:  # Start checking after 10 epochs\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# --- Plot train/val losses ---\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plt.plot(train_losses, label='Train Loss')\n",
    "# plt.plot(val_losses, label='Val Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.show()\n",
    "\n",
    "# --- Confusion matrix for each horizon ---\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        out = model(xb)  # (batch, n_horizons, n_classes)\n",
    "        preds = out.argmax(-1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(yb.numpy())\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# class_names = ['<-7%', '-7%~-3%', '-3%~-1%', '-1%~+1%', '+1%~+3%', '+3%~+7%', '>+7%']\n",
    "# for h in range(n_horizons):\n",
    "#     cm = confusion_matrix(all_targets[:,h], all_preds[:,h], labels=list(range(7)))\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "#     disp.plot(cmap='Blues')\n",
    "#     plt.title(f'Confusion Matrix - Horizon {h+1}')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfc610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
